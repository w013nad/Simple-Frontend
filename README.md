# The simplest LLM frontend

## Launch the LLM
```
llama-server.exe -m Qwen2.5-0.5B-Instruct.Q4_K_M.gguf -ngl 999
```

## Open the HTML file in any browser

Note that this expects that you are using llama.cpp with an openai style endpoint
